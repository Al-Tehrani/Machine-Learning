{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/banner.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Estimation in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter estimation stands at the very heart of machine learning, serving as the cornerstone upon which most algorithms are built. At its core, machine learning is fundamentally about discovering the optimal parameters for functions that can accurately describe and predict real-world phenomena.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From classic algorithms like Linear Regression and Naive Bayes to cutting-edge Deep Neural Networks, the fundamental goal remains consistent: estimating parameters that best fit the observed data and capture underlying patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our previous studies of probability theory, we encountered various distributions for random variables. Each of these distributions was characterized by specific parameters - the numerical inputs that define the behavior of the random variable. Up until now, we've worked with scenarios where these parameters were either explicitly provided or could be inferred from our understanding of the underlying process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, real-world machine learning problems often present a different challenge:\n",
    "\n",
    "- We may not know the values of the parameters.\n",
    "- We can't estimate them solely from expert knowledge.\n",
    "- Instead of knowing the random variables, we have a large dataset generated from an unknown underlying distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where parameter estimation comes into play. In this chapter, we will explore formal methods for estimating parameters from data, bridging the gap between theoretical probability models and practical machine learning applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a simple example to illustrate the concept of parameter estimation. Imagine we're trying to predict a person's weight based on their height using linear regression. Our model might look like this:\n",
    "\n",
    "$$ \\text{Weight} = \\beta_0 + \\beta_1 \\cdot \\text{Height} + \\epsilon $$\n",
    "\n",
    "Where:\n",
    "- $\\beta_0$ is the y-intercept (base weight)\n",
    "- $\\beta_1$ is the slope (weight increase per unit of height)\n",
    "- $\\epsilon$ is the error term\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, $\\beta_0$ and $\\beta_1$ are our parameters. We don't know their true values, but we can estimate them from a dataset of height-weight measurements. The process of finding the best values for $\\beta_0$ and $\\beta_1$ that fit our data is parameter estimation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By mastering parameter estimation techniques, you'll gain the ability to:\n",
    "\n",
    "1. **Extract meaningful information** from complex datasets\n",
    "2. **Fit models** that accurately represent underlying patterns\n",
    "3. **Make predictions** based on learned parameters\n",
    "4. **Understand the uncertainty** associated with your estimates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we dive deeper into this crucial topic, you'll see how parameter estimation forms the foundation for many of the machine learning concepts we'll explore in future lectures. Let's embark on this journey to unravel the power of parameter estimation in machine learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [What is Parameter Estimation?](#toc1_)    \n",
    "  - [Definition](#toc1_1_)    \n",
    "  - [Key Components](#toc1_2_)    \n",
    "  - [Example: Gaussian Distribution](#toc1_3_)    \n",
    "  - [The Estimation Process](#toc1_4_)    \n",
    "  - [Why It Matters](#toc1_5_)    \n",
    "- [Types of Parameters in Machine Learning Models](#toc2_)    \n",
    "  - [Model Parameters](#toc2_1_)    \n",
    "  - [Hyperparameters](#toc2_2_)    \n",
    "  - [Latent Parameters](#toc2_3_)    \n",
    "  - [Fixed Parameters](#toc2_4_)    \n",
    "  - [Structural Parameters](#toc2_5_)    \n",
    "  - [Regularization Parameters](#toc2_6_)    \n",
    "- [Overview of Parameter Estimation Techniques](#toc3_)    \n",
    "  - [Maximum Likelihood Estimation (MLE)](#toc3_1_)    \n",
    "  - [Bayesian Estimation](#toc3_2_)    \n",
    "  - [Maximum A Posteriori (MAP) Estimation](#toc3_3_)    \n",
    "  - [Method of Moments](#toc3_4_)    \n",
    "- [Conclusion](#toc4_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_'></a>[What is Parameter Estimation?](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter estimation is a fundamental process in statistics and machine learning where we attempt to determine the most likely values of parameters for a given model based on observed data. In essence, it's the bridge that connects our theoretical models to real-world observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_'></a>[Definition](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, parameter estimation can be defined as:\n",
    "\n",
    "> The process of using sample data to calculate a numerical value (an estimate) for an unknown population parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning contexts, we can expand this definition:\n",
    "\n",
    "> The task of finding the best set of parameters for a model that maximizes its ability to explain or predict the observed data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_'></a>[Key Components](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Model**: A mathematical representation of the relationship between variables in our data.\n",
    "2. **Parameters**: The unknown values in our model that we need to estimate.\n",
    "3. **Data**: The observed samples from which we estimate the parameters.\n",
    "4. **Estimation Method**: The algorithm or approach used to find the best parameter values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_'></a>[Example: Gaussian Distribution](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a concrete example to illustrate parameter estimation. Suppose we have a dataset that we believe follows a Gaussian (Normal) distribution. The Gaussian distribution is defined by two parameters:\n",
    "\n",
    "- $\\mu$ (mu): the mean\n",
    "- $\\sigma$ (sigma): the standard deviation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability density function of a Gaussian distribution is:\n",
    "\n",
    "$$ f(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, parameter estimation involves finding the values of $\\mu$ and $\\sigma$ that best describe our observed data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_'></a>[The Estimation Process](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Collect Data**: Gather a sample of observations.\n",
    "2. **Choose a Model**: In this case, we've chosen the Gaussian distribution.\n",
    "3. **Select an Estimation Method**: We might use Maximum Likelihood Estimation (MLE) or Method of Moments.\n",
    "4. **Estimate Parameters**: Apply the chosen method to find $\\hat{\\mu}$ and $\\hat{\\sigma}$ (the \"hat\" notation denotes estimates).\n",
    "5. **Evaluate**: Assess how well our estimated parameters fit the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_'></a>[Why It Matters](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter estimation is crucial because it allows us to:\n",
    "\n",
    "- **Understand Data**: By estimating parameters, we gain insights into the underlying structure of our data.\n",
    "- **Make Predictions**: Once we have estimated parameters, we can use our model to make predictions on new, unseen data.\n",
    "- **Quantify Uncertainty**: Many estimation methods also provide measures of uncertainty around our estimates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we progress through this course, you'll see how parameter estimation forms the backbone of many machine learning algorithms, from simple linear regression to complex neural networks. Understanding this concept is key to mastering the art and science of machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_'></a>[Types of Parameters in Machine Learning Models](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, we encounter various types of parameters across different models. Understanding these parameter types is crucial for effective model design, training, and interpretation. Let's explore the main categories:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_'></a>[Model Parameters](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the parameters learned from the data during the training process. They define the model's behavior and are updated iteratively to minimize the loss function. Model parameters are specific to the chosen algorithm and represent the learned patterns in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Characteristics**:\n",
    "  - Estimated from the training data\n",
    "  - Define the model's learned behavior\n",
    "  - Updated iteratively during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Examples**:\n",
    "  - Weights and biases in neural networks\n",
    "  - Coefficients in linear regression\n",
    "  - Mean and variance in Gaussian Naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Example: Linear Regression parameters\n",
    "y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, β₀, β₁, β₂, ..., βₙ are model parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_'></a>[Hyperparameters](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are configuration variables that are set before the learning process begins. Hyperparameters control the learning process and model behavior, impacting performance and generalization. They are not learned from the data but are tuned based on validation results or domain knowledge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Characteristics**:\n",
    "  - Not learned from the data\n",
    "  - Control the learning process\n",
    "  - Often tuned using techniques like cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Examples**:\n",
    "  - Learning rate in gradient descent\n",
    "  - Number of hidden layers in a neural network\n",
    "  - Regularization strength in regularized models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Example: Hyperparameter in scikit-learn\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(C=1.0, kernel='rbf')  # C and kernel are hyperparameters\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_'></a>[Latent Parameters](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are hidden or unobserved parameters that the model infers from the data. Latent parameters capture underlying patterns or structures that are not directly observed but are essential for modeling complex relationships. They are often used in probabilistic models to represent hidden variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Characteristics**:\n",
    "  - Not directly observed in the data\n",
    "  - Inferred during the learning process\n",
    "  - Often represent underlying structure or factors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Examples**:\n",
    "  - Topic distributions in Latent Dirichlet Allocation (LDA)\n",
    "  - Hidden states in Hidden Markov Models (HMM)\n",
    "  - Latent factors in matrix factorization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_'></a>[Fixed Parameters](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are parameters that are fixed and not learned during training. Fixed parameters are predefined and remain constant throughout the learning process. They are often based on prior knowledge, constraints, or design choices. Fixed parameters can significantly impact the model's architecture and behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Characteristics**:\n",
    "  - Predetermined and constant\n",
    "  - Not updated during training\n",
    "  - Often based on domain knowledge or constraints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Examples**:\n",
    "  - Convolutional filter sizes in CNNs\n",
    "  - Activation function choices in neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_'></a>[Structural Parameters](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These define the structure or architecture of the model. Structural parameters determine the model's capacity and form, influencing its complexity and representational power. They are set before training and can have a significant impact on the model's performance. Structural parameters are often chosen based on the problem domain and computational constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Characteristics**:\n",
    "  - Determine the model's capacity and form\n",
    "  - Usually set before training\n",
    "  - Can significantly impact model complexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Examples**:\n",
    "  - Number of neurons per layer in a neural network\n",
    "  - Degree of a polynomial regression model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Example: Structural parameter in Keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(10,)),  # 64 is a structural parameter\n",
    "    Dense(1)\n",
    "])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_6_'></a>[Regularization Parameters](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These control the model's complexity and prevent overfitting. Regularization parameters are used to penalize large weights or complex models, encouraging simpler solutions that generalize better. They are crucial for balancing model fit and complexity, improving performance on unseen data. Regularization parameters are often tuned through cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Characteristics**:\n",
    "  - Influence the trade-off between model fit and simplicity\n",
    "  - Often treated as hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Examples**:\n",
    "  - λ in L1 (Lasso) or L2 (Ridge) regularization\n",
    "  - Dropout rate in neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Example: Regularization parameter in scikit-learn\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=1.0)  # alpha is a regularization parameter\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding these different types of parameters is essential for:\n",
    "- Properly designing and implementing machine learning models\n",
    "- Effectively tuning and optimizing model performance\n",
    "- Interpreting model results and understanding their limitations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you work with various machine learning algorithms, you'll encounter these different parameter types, and knowing how to handle each type will be crucial for successful model development and deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_'></a>[Overview of Parameter Estimation Techniques](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out there isn’t just one way to estimate the value of parameters. There are two main schools of thought: frequentist and Bayesian. In the frequentist approach, we treat parameters as fixed but unknown values that we aim to estimate. In the Bayesian approach, we treat parameters as random variables with their own distributions. oth of these schools\n",
    "of thought assume that your data are independent and identically distributed (i.i.d.). This means that each data point is drawn from the same distribution and is independent of the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dive deeper into these three fundamental parameter estimation techniques, providing more intuitive explanations and relating them to different statistical philosophies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_'></a>[Maximum Likelihood Estimation (MLE)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistical Philosophy**: Frequentist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuitive Explanation**: \n",
    "Imagine you're trying to guess the rules of a game by watching many rounds. MLE is like choosing the set of rules that would make the observed outcomes most likely. It asks, \"What parameter values would make our observed data most probable?\" It's a frequentist approach that treats parameters as fixed but unknown values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLE seeks to find the parameter values that maximize the probability of observing the given data. It's based on the likelihood function, which expresses how likely the observed data is as a function of the parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a dataset X = {x₁, x₂, ..., xₙ} and parameters θ, the likelihood function is:\n",
    "\n",
    "$$ L(\\theta|X) = P(X|\\theta) = \\prod_{i=1}^n P(x_i|\\theta) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often work with the log-likelihood for computational convenience:\n",
    "\n",
    "$$ \\ell(\\theta|X) = \\log L(\\theta|X) = \\sum_{i=1}^n \\log P(x_i|\\theta) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLE finds the parameters that maximize this function:\n",
    "\n",
    "$$ \\hat{\\theta}_{MLE} = \\arg\\max_{\\theta} \\ell(\\theta|X) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: \n",
    "In a coin flipping experiment, MLE would estimate the probability of heads by choosing the value that makes the observed sequence of flips most likely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/prob-likelihood.webp\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros**:\n",
    "- Often leads to consistent and efficient estimators\n",
    "- Widely applicable and computationally tractable\n",
    "\n",
    "**Cons**:\n",
    "- Can be biased for small samples\n",
    "- Doesn't incorporate prior knowledge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_'></a>[Bayesian Estimation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistical Philosophy**: Bayesian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuitive Explanation**: \n",
    "Imagine you're a detective solving a case. You start with some initial hunches (prior beliefs) about the culprit. As you gather evidence (data), you update your suspicions. Bayesian estimation works similarly, starting with prior beliefs about parameters and updating them based on observed data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detailed Explanation**:\n",
    "Bayesian estimation uses Bayes' theorem to combine prior beliefs about parameters with observed data to produce a posterior distribution over the parameters.\n",
    "\n",
    "Bayes' theorem states:\n",
    "\n",
    "$$ P(\\theta|X) = \\frac{P(X|\\theta)P(\\theta)}{P(X)} $$\n",
    "\n",
    "Where:\n",
    "- $P(\\theta|X)$ is the posterior distribution (updated beliefs about parameters)\n",
    "- $P(X|\\theta)$ is the likelihood (as in MLE)\n",
    "- $P(\\theta)$ is the prior distribution (initial beliefs about parameters)\n",
    "- $P(X)$ is the evidence (a normalizing constant)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of a point estimate, Bayesian estimation provides a full distribution over possible parameter values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**:\n",
    "In estimating the skill of a chess player, we might start with a prior belief based on their official rating. After observing several games, we update this belief to form a posterior distribution of their true skill.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros**:\n",
    "- Incorporates prior knowledge\n",
    "- Provides uncertainty quantification\n",
    "- Works well with small sample sizes\n",
    "\n",
    "**Cons**:\n",
    "- Can be computationally intensive\n",
    "- Choice of prior can be subjective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_3_'></a>[Maximum A Posteriori (MAP) Estimation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistical Philosophy:** Bayesian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAP estimation is a Bayesian approach to parameter estimation, combining prior knowledge with observed data. It seeks to find the parameter values that maximize the posterior probability of the parameters given the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you're a detective trying to solve a case. You start with some initial hunches (prior beliefs) about the culprit. As you gather evidence (data), you update your suspicions. MAP estimation is like choosing the suspect that is most likely given both your initial hunches and the new evidence. It's a balance between prior beliefs and observed data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How MAP Works:**\n",
    "1. **Start with a Prior**: Begin with a prior probability distribution over the parameters, P(θ).\n",
    "\n",
    "2. **Incorporate Data**: Use the likelihood of the data given the parameters, P(X|θ).\n",
    "\n",
    "3. **Apply Bayes' Theorem**: Combine prior and likelihood to get the posterior probability:\n",
    "\n",
    "   P(θ|X) ∝ P(X|θ) * P(θ)\n",
    "\n",
    "4. **Find the Maximum**: The MAP estimate is the value of θ that maximizes this posterior probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAP estimation finds:\n",
    "\n",
    "$$ \\theta_{MAP} = \\argmax_{\\theta} P(\\theta|X) = \\argmax_{\\theta} [P(X|\\theta) \\cdot P(\\theta)] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, we work with the log posterior for computational convenience:\n",
    "\n",
    "$$ \\theta_{MAP} = \\argmax_{\\theta} [\\log P(X|\\theta) + \\log P(\\theta)] $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Comparison with Other Methods:**\n",
    "1. **vs. Maximum Likelihood Estimation (MLE)**:\n",
    "   - MLE maximizes only the likelihood: θ_MLE = argmax_θ P(X|θ)\n",
    "   - MAP includes the prior: θ_MAP = argmax_θ [P(X|θ) * P(θ)]\n",
    "   - When the prior is uniform, MAP and MLE give the same result.\n",
    "\n",
    "2. **vs. Full Bayesian Estimation**:\n",
    "   - Full Bayesian estimation provides the entire posterior distribution.\n",
    "   - MAP gives a point estimate (the mode of the posterior).\n",
    "   - MAP can be seen as an approximation to full Bayesian inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros of MAP Estimation:**\n",
    "1. **Incorporates Prior Knowledge**: Useful when you have reliable prior information.\n",
    "2. **Regularization Effect**: The prior can help prevent overfitting, especially with small datasets.\n",
    "3. **Point Estimate**: Provides a single \"best\" estimate, which can be more practical in some applications.\n",
    "\n",
    "**Cons of MAP Estimation:**\n",
    "1. **Sensitivity to Prior**: Results can be heavily influenced by the choice of prior.\n",
    "2. **Point Estimate Only**: Doesn't provide full uncertainty quantification like full Bayesian methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/map-mle.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "Suppose we're estimating the probability of a coin landing heads. We might use a Beta prior (which is conjugate to the Binomial likelihood for a coin flip):\n",
    "\n",
    "- Prior: $\\theta \\sim \\text{Beta}(\\alpha, \\beta)$\n",
    "- Likelihood: $X \\sim \\text{Binomial}(n, \\theta)$\n",
    "- Posterior: $\\theta|X \\sim \\text{Beta}(\\alpha + \\text{heads}, \\beta + \\text{tails})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MAP estimate would be the mode of this Beta posterior distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAP estimation bridges the gap between frequentist methods like MLE and full Bayesian approaches. It incorporates prior knowledge while still providing a point estimate, making it a valuable tool in many machine learning and statistical inference tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_4_'></a>[Method of Moments](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistical Philosophy:** Frequentist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Method of Moments is a parameter estimation technique that belongs to the frequentist school of thought in statistics. It's based on the idea of using sample moments to estimate population parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you're trying to guess the recipe for a cake by tasting it. The Method of Moments is like matching the characteristics of your cake (sweetness, texture) to the known effects of different ingredients. In statistical terms, you're matching the \"moments\" of your sample data to the theoretical moments of the distribution you believe the data follows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are Moments?\n",
    "1. **Population Moments**: These are expected values of different powers of a random variable.\n",
    "   - First moment: E[X] (mean)\n",
    "   - Second moment: E[X²]\n",
    "   - Third moment: E[X³]\n",
    "   - And so on...\n",
    "\n",
    "2. **Sample Moments**: These are the observed equivalents in your data.\n",
    "   - First sample moment: x̄ (sample mean)\n",
    "   - Second sample moment: average of x²\n",
    "   - And so on...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the Method of Moments work?\n",
    "1. **Express Population Moments in Terms of Parameters**:\n",
    "   For a given distribution, express the theoretical moments in terms of the parameters you want to estimate.\n",
    "\n",
    "   E[X] = f(parameters)\n",
    "   E[X²] = g(parameters)\n",
    "   ...\n",
    "\n",
    "2. **Equate Sample Moments to Population Moments**:\n",
    "   Set up equations where sample moments equal their theoretical counterparts.\n",
    "\n",
    "   x̄ ≈ E[X] = f(parameters)\n",
    "   (1/n)Σx²ᵢ ≈ E[X²] = g(parameters)\n",
    "   ...\n",
    "\n",
    "3. **Solve for Parameters**:\n",
    "   Solve these equations to get estimates for your parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Uniform Distribution\n",
    "Let's estimate parameters $a$ and $b$ for a uniform distribution on interval $[a, b]$.\n",
    "\n",
    "1. **Theoretical Moments**:\n",
    "   $E[X] = \\frac{a + b}{2}$\n",
    "   $E[X^2] = \\frac{a^2 + ab + b^2}{3}$\n",
    "\n",
    "2. **Equate to Sample Moments**:\n",
    "   $\\bar{x} = \\frac{a + b}{2}$\n",
    "   $\\frac{1}{n}\\sum_{i=1}^n x_i^2 = \\frac{a^2 + ab + b^2}{3}$\n",
    "\n",
    "3. **Solve**:\n",
    "   From these equations, we can solve for $a$ and $b$ to get our estimates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key advantage of the Method of Moments is its simplicity and universality. It can be applied even when likelihood functions are hard to work with, making it a valuable tool in the statistician's toolkit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros**:\n",
    "1. **Simplicity**: Often leads to straightforward calculations.\n",
    "2. **Universality**: Can be applied even when the likelihood function is hard to work with.\n",
    "\n",
    "**Cons**:\n",
    "1. **Efficiency**: Not always the most efficient estimator, especially for small samples.\n",
    "2. **Potential Issues**: Can sometimes produce estimates outside the parameter space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of parameter estimation method depends on various factors, including the nature of the data, the model complexity, and the computational resources available. Here's a brief comparison of the three methods:\n",
    "- **vs. MLE**: Generally simpler but often less efficient.\n",
    "- **vs. Bayesian**: Doesn't incorporate prior information or provide a full posterior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The choice between them often depends on the specific problem, available data, computational resources, and philosophical preferences. In practice, statisticians and data scientists may use a combination of these methods or choose based on the particular requirements of their analysis. Here's a comparison table of the parameter estimation methods we've discussed:\n",
    "\n",
    "| Aspect | Maximum Likelihood Estimation (MLE) | Bayesian Estimation | Method of Moments (MoM) | Maximum A Posteriori (MAP) |\n",
    "|--------|-------------------------------------|---------------------|-------------------------|----------------------------|\n",
    "| **Philosophy** | Frequentist | Bayesian | Frequentist | Bayesian |\n",
    "| **Core Idea** | Maximize the likelihood of observed data | Update prior beliefs with observed data | Match sample moments to theoretical moments | Maximize the posterior probability |\n",
    "| **Prior Information** | Not used | Incorporated | Not used | Incorporated |\n",
    "| **Result** | Point estimate | Full posterior distribution | Point estimate | Point estimate |\n",
    "| **Objective Function** | Likelihood: P(X\\|θ) | Posterior: P(θ\\|X) | Moment equations | Posterior: P(θ\\|X) |\n",
    "| **Computation** | Often analytically tractable | Can be computationally intensive | Usually simple | Often simpler than full Bayesian |\n",
    "| **Uncertainty Quantification** | Requires additional methods (e.g., bootstrapping) | Natural part of the method | Not directly provided | Not directly provided |\n",
    "| **Sample Size Sensitivity** | Performs well with large samples | Can work well with small samples if prior is informative | Can be unreliable with small samples | Can work well with small samples if prior is informative |\n",
    "| **Efficiency** | Often most efficient (asymptotically) | Can be more efficient with informative priors | Generally less efficient than MLE | Often more efficient than MLE with informative priors |\n",
    "| **Bias** | Can be biased for small samples | Can be biased if prior is poorly chosen | Can be biased | Can be biased if prior is poorly chosen |\n",
    "| **Consistency** | Typically consistent | Typically consistent | Typically consistent | Typically consistent |\n",
    "| **Handling Complex Models** | Can be challenging for very complex models | Can handle complex models via MCMC methods | Often simpler for complex models | Can handle moderately complex models |\n",
    "| **Main Advantage** | Efficiency and wide applicability | Full uncertainty quantification | Simplicity and universality | Incorporates prior knowledge with point estimate |\n",
    "| **Main Limitation** | Doesn't use prior information | Can be computationally intensive | Not always efficient | Sensitive to prior choice |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_'></a>[Conclusion](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter estimation is a fundamental task in statistics and machine learning, serving as the bridge between theoretical models and real-world data. As we've explored, there are several approaches to this crucial task, each with its own strengths, limitations, and philosophical underpinnings. Let's summarize the key takeaways from this chapter:\n",
    "\n",
    "1. **Diversity of Methods**: From the simplicity of the Method of Moments to the comprehensive uncertainty quantification of full Bayesian estimation, each approach offers unique advantages.\n",
    "\n",
    "2. **Philosophical Divide**: The frequentist (MLE, MoM) and Bayesian (MAP, full Bayesian) approaches represent different ways of thinking about probability and inference.\n",
    "\n",
    "3. **Trade-offs**: Each method involves trade-offs between computational complexity, incorporation of prior knowledge, efficiency, and interpretability.\n",
    "\n",
    "4. **Contextual Choice**: The best method often depends on the specific context, including sample size, model complexity, available prior information, and computational resources.\n",
    "\n",
    "5. **Complementary Nature**: These methods are not always competing; they can be complementary. For instance, MLE estimates might serve as starting points for Bayesian analyses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When choosing a parameter estimation method, consider the following factors:\n",
    "- **Small Samples**: Bayesian methods (including MAP) can be particularly valuable when dealing with small sample sizes, especially if informative priors are available.\n",
    "- **Large Datasets**: MLE often shines with large datasets, providing efficient and consistent estimates.\n",
    "- **Computational Constraints**: Method of Moments might be preferred when quick, rough estimates are needed, or when dealing with very complex models where other methods are intractable.\n",
    "- **Uncertainty Quantification**: Full Bayesian estimation is unparalleled when a complete picture of parameter uncertainty is required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As machine learning and statistical methods continue to evolve, we're likely to see:\n",
    "- More sophisticated hybrid approaches that combine the strengths of different estimation techniques.\n",
    "- Increased use of approximate Bayesian methods to balance the benefits of Bayesian inference with computational efficiency.\n",
    "- Greater emphasis on interpretable and explainable models, where the choice of estimation method plays a crucial role in model interpretation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mastering these parameter estimation techniques equips data scientists and statisticians with a versatile toolkit for tackling a wide range of real-world problems. The ability to choose and apply the appropriate method for a given scenario is a hallmark of expertise in the field.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you continue your journey in machine learning and statistics, remember that these methods are not just theoretical constructs but practical tools that form the foundation of data-driven decision-making across numerous domains. By understanding their strengths and limitations, you'll be well-prepared to extract meaningful insights from data and build robust, reliable models."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
