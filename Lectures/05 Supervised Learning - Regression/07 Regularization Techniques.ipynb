{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/banner.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization Techniques in Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is a fundamental concept in machine learning that addresses one of the most common challenges in model building: creating models that generalize well to unseen data. As we delve into more complex models, the risk of overfitting increases, and regularization provides a powerful set of techniques to mitigate this risk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization refers to a set of techniques that prevent overfitting by adding a penalty term to the loss function. This penalty discourages the model from becoming too complex, effectively constraining its capacity to memorize the training data. Regularization helps to create models that are simpler and more generalizable, striking a balance between fitting the training data and maintaining predictive power on new, unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is Regularization Important?\n",
    "\n",
    "1. **Prevents Overfitting:** By limiting model complexity, regularization helps avoid the model fitting noise in the training data.\n",
    "\n",
    "2. **Improves Generalization:** Regularized models often perform better on unseen data, making them more reliable in real-world applications.\n",
    "\n",
    "3. **Feature Selection:** Some regularization techniques can help identify the most important features, leading to more interpretable models.\n",
    "\n",
    "4. **Handling High-Dimensional Data:** Regularization is particularly useful when dealing with datasets where the number of features is large relative to the number of samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, we'll focus on three main types of regularization for linear regression:\n",
    "\n",
    "1. Ridge Regression (L2 Regularization)\n",
    "2. Lasso Regression (L1 Regularization)\n",
    "3. Elastic Net Regularization (Combination of L1 and L2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/regularization.webp\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these techniques adds a different type of penalty to the model, resulting in different effects on the model's behavior and the resulting coefficients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is not just a technique for linear regression; it's a fundamental concept that extends to many areas of machine learning, including:\n",
    "\n",
    "- Logistic Regression\n",
    "- Neural Networks (weight decay, dropout)\n",
    "- Support Vector Machines\n",
    "- And many more advanced models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 **Pro Tip:** Understanding regularization in the context of linear regression provides a solid foundation for grasping more complex regularization techniques in advanced machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections, we'll explore the problem of overfitting, delve into the bias-variance trade-off, and then examine each regularization technique in detail. We'll also look at practical implementations and guidelines for choosing the right regularization approach for your specific problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While regularization is a powerful tool, it's not a silver bullet. It's crucial to understand when and how to apply these techniques effectively. By the end of this lecture, you'll have a comprehensive understanding of regularization techniques, their impact on model performance, and how to implement them in your own machine learning projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [The Problem of Overfitting](#toc1_)    \n",
    "  - [Symptoms of Overfitting](#toc1_1_)    \n",
    "  - [Causes of Overfitting](#toc1_2_)    \n",
    "  - [The Importance of Addressing Overfitting](#toc1_3_)    \n",
    "- [Understanding the Bias-Variance Trade-off](#toc2_)    \n",
    "  - [Decomposing Prediction Error](#toc2_1_)    \n",
    "  - [High Bias vs. High Variance Models](#toc2_2_)    \n",
    "  - [The Trade-off Curve](#toc2_3_)    \n",
    "  - [Implications for Regularization](#toc2_4_)    \n",
    "  - [Practical Considerations](#toc2_5_)    \n",
    "- [Ridge Regression (L2 Regularization)](#toc3_)    \n",
    "  - [Deriving the Ridge Regression Objective](#toc3_1_)    \n",
    "  - [How Ridge Regression Works](#toc3_2_)    \n",
    "  - [Choosing the Regularization Parameter](#toc3_3_)    \n",
    "  - [Implementing Ridge Regression](#toc3_4_)    \n",
    "- [Lasso Regression (L1 Regularization)](#toc4_)    \n",
    "  - [How Lasso Regression Works](#toc4_1_)    \n",
    "  - [The Path of Coefficients](#toc4_2_)    \n",
    "  - [Choosing the Regularization Parameter](#toc4_3_)    \n",
    "  - [Advantages and Limitations](#toc4_4_)    \n",
    "  - [Implementing Lasso Regression](#toc4_5_)    \n",
    "  - [Practical Considerations](#toc4_6_)    \n",
    "- [Elastic Net Regularization](#toc5_)    \n",
    "  - [Mathematical Formulation](#toc5_1_)    \n",
    "  - [How Elastic Net Works](#toc5_2_)    \n",
    "  - [Choosing Hyperparameters](#toc5_3_)    \n",
    "  - [Advantages and Limitations](#toc5_4_)    \n",
    "  - [Implementing Elastic Net](#toc5_5_)    \n",
    "  - [Practical Considerations](#toc5_6_)    \n",
    "- [Impact of Regularization on Bias and Variance](#toc6_)    \n",
    "  - [Impact of Ridge Regression (L2)](#toc6_1_)    \n",
    "  - [Impact of Lasso Regression (L1)](#toc6_2_)    \n",
    "  - [Impact of Elastic Net](#toc6_3_)    \n",
    "  - [Practical Implications](#toc6_4_)    \n",
    "  - [Visualization of the Impact](#toc6_5_)    \n",
    "- [Summary and Key Takeaways](#toc7_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_'></a>[The Problem of Overfitting](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting is a fundamental challenge in machine learning that occurs when a model learns the training data too well, including its noise and peculiarities, at the expense of generalizing to new, unseen data. Understanding overfitting is crucial for developing effective machine learning models, as it directly impacts a model's real-world performance and reliability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting occurs when a model captures not just the underlying patterns in the data, but also the random fluctuations and noise. As a result, the model performs exceptionally well on the training data but fails to generalize to new, unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/overfitting.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔑 **Key Concept:** An overfit model is like memorizing answers to a test rather than understanding the underlying concepts. It performs well on the known questions but struggles with new, slightly different problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_'></a>[Symptoms of Overfitting](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recognizing overfitting is crucial for model evaluation and improvement. Here are some common signs:\n",
    "\n",
    "1. **Large Gap Between Training and Test Performance:** \n",
    "   When a model shows significantly better performance on the training data compared to the test data, it's a strong indicator of overfitting. This gap suggests that the model has learned patterns specific to the training set that don't generalize well.\n",
    "\n",
    "2. **Excessive Model Complexity:** \n",
    "   Overly complex models with many parameters relative to the amount of training data are prone to overfitting. In linear regression, this might manifest as coefficients with very large absolute values or a model that uses many features to explain a simple relationship.\n",
    "\n",
    "3. **Poor Performance on New Data:** \n",
    "   The ultimate test of a model is its performance on new, unseen data. If a model that performs well during training fails to make accurate predictions on new data, overfitting is likely the culprit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_'></a>[Causes of Overfitting](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the root causes of overfitting can help in developing strategies to prevent it:\n",
    "\n",
    "1. **Limited Data:** \n",
    "   When the training dataset is too small, the model might learn the noise in the data rather than the true underlying pattern. This is particularly problematic in high-dimensional spaces where the number of features is large compared to the number of samples.\n",
    "\n",
    "2. **Model Complexity:** \n",
    "   Models with high complexity (e.g., many parameters, high-degree polynomials in regression) have the capacity to fit the training data very closely, including its noise. While this results in low training error, it often leads to poor generalization.\n",
    "\n",
    "3. **Noise in the Data:** \n",
    "   Real-world data often contains noise — random fluctuations or errors in measurement. An overfit model might interpret this noise as a pattern to be learned, leading to poor generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_'></a>[The Importance of Addressing Overfitting](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tackling overfitting is crucial for several reasons:\n",
    "\n",
    "- **Model Reliability:** Overfit models are unreliable in real-world applications, as their performance on new data can be unpredictable.\n",
    "- **Resource Efficiency:** Simpler models that generalize well are often more computationally efficient and easier to maintain.\n",
    "- **Interpretability:** Non-overfit models tend to be more interpretable, providing clearer insights into the underlying relationships in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🤔 **Why This Matters:** In practical machine learning, the goal is rarely to achieve perfect performance on the training data. Instead, we aim for models that generalize well, making reliable predictions on new, unseen data. Understanding and addressing overfitting is key to achieving this goal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next sections, we'll explore the bias-variance trade-off, which provides a theoretical framework for understanding overfitting, and then delve into regularization techniques that help combat this problem in linear regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_'></a>[Understanding the Bias-Variance Trade-off](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias-variance trade-off is a fundamental concept in machine learning that provides a framework for understanding model error and the problem of overfitting. It helps us balance the complexity of our model against its ability to generalize to new data. This trade-off is crucial in choosing the right model and in applying regularization techniques effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_'></a>[Decomposing Prediction Error](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the bias-variance trade-off, we first need to break down the sources of error in our predictions. The expected prediction error of a model can be decomposed into three components:\n",
    "\n",
    "1. **Bias:** The error introduced by approximating a real-world problem with a simplified model.\n",
    "2. **Variance:** The error due to the model's sensitivity to fluctuations in the training data.\n",
    "3. **Irreducible Error:** The inherent noise in the problem that cannot be reduced by any model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, this decomposition is often expressed as:\n",
    "\n",
    "$$ \\text{Expected Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/bias-variance-tradeoff.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/bias-variance.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔑 **Key Concept:** The goal in model selection is to find the sweet spot that minimizes both bias and variance, recognizing that reducing one often comes at the cost of increasing the other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_'></a>[High Bias vs. High Variance Models](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the characteristics of high bias and high variance models helps in diagnosing and addressing model performance issues:\n",
    "\n",
    "1. **High Bias (Underfitting)**\n",
    "   - Symptoms: Poor performance on both training and test data\n",
    "   - Characteristics: \n",
    "     - Too simple to capture the underlying pattern in the data\n",
    "     - Makes strong assumptions about the data distribution\n",
    "     - Examples: Linear models for complex, non-linear relationships\n",
    "\n",
    "   🔍 **Example:** Using a linear regression to model a clearly quadratic relationship would result in high bias.\n",
    "\n",
    "2. **High Variance (Overfitting)**\n",
    "   - Symptoms: Excellent performance on training data, poor performance on test data\n",
    "   - Characteristics:\n",
    "     - Too complex, capturing noise in the training data\n",
    "     - Highly sensitive to small fluctuations in the training set\n",
    "     - Examples: High-degree polynomial models, deep neural networks with many parameters\n",
    "\n",
    "   🔍 **Example:** Fitting a high-degree polynomial to a dataset with just a few points would likely result in high variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_'></a>[The Trade-off Curve](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between model complexity, bias, and variance can be visualized with a characteristic U-shaped curve:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/bias-variance-2.jpeg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As model complexity increases:\n",
    "- Bias typically decreases (the model can fit the data more closely)\n",
    "- Variance typically increases (the model becomes more sensitive to the specific training data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal model complexity occurs at the point where the sum of squared bias and variance is minimized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_'></a>[Implications for Regularization](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the bias-variance trade-off is crucial for effective regularization:\n",
    "\n",
    "- Regularization techniques generally work by increasing bias slightly to achieve a larger reduction in variance.\n",
    "- The goal is to move along the trade-off curve to find the optimal balance for a given problem and dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 **Pro Tip:** When applying regularization, monitor both training and validation performance. If both improve, you're likely reducing variance without significantly increasing bias – a win-win situation!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_'></a>[Practical Considerations](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, navigating the bias-variance trade-off involves:\n",
    "- Careful feature selection and engineering\n",
    "- Choosing an appropriate model complexity\n",
    "- Using cross-validation to estimate the true error\n",
    "- Applying regularization techniques judiciously\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Important Note:** The optimal trade-off point depends on your specific problem, the amount of data available, and the cost associated with different types of errors in your application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By understanding the bias-variance trade-off, you'll be better equipped to diagnose model performance issues, choose appropriate regularization techniques, and ultimately build models that generalize well to new, unseen data. In the following sections, we'll explore specific regularization techniques that help manage this trade-off in the context of linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_'></a>[Ridge Regression (L2 Regularization)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression, also known as L2 regularization, is a powerful technique used to mitigate overfitting in linear regression models. It can be derived from a probabilistic perspective, providing insights into its underlying assumptions and behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This regularization technique is called L2 because it adds a penalty term to the loss function that corresponds to the squared magnitude of the coefficients. This penalty encourages the model to keep the coefficients small, effectively shrinking them towards zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression can be understood as a Maximum A Posteriori (MAP) estimation under specific probabilistic assumptions:\n",
    "\n",
    "1. **Likelihood**: We assume the target variable follows a normal distribution around the model's prediction:\n",
    "\n",
    "   $y | X, \\beta \\sim \\mathcal{N}(X\\beta, \\sigma^2I)$\n",
    "\n",
    "2. **Prior**: We assume a Gaussian prior on the weights:\n",
    "\n",
    "   $\\beta \\sim \\mathcal{N}(0, \\tau^2I)$\n",
    "\n",
    "Here, $\\sigma^2$ represents the noise variance, and $\\tau^2$ controls the spread of the prior on weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/normal.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_'></a>[Deriving the Ridge Regression Objective](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Bayes' theorem, the posterior distribution of $\\beta$ given the data is:\n",
    "\n",
    "$p(\\beta | X, y) \\propto p(y | X, \\beta) \\cdot p(\\beta)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the negative log of this posterior (to convert multiplication to addition and maximize to minimize):\n",
    "\n",
    "$\\log p(\\beta | X, y) \\propto \\log p(y | X, \\beta) + \\log p(\\beta)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding these terms:\n",
    "\n",
    "1. $\\log p(y | X, \\beta) \\propto \\frac{1}{2\\sigma^2} \\|y - X\\beta\\|^2_2$\n",
    "2. $\\log p(\\beta) \\propto \\frac{1}{2\\tau^2} \\|\\beta\\|^2_2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining these and dropping constants, we get the Ridge regression objective:\n",
    "\n",
    "$\\min_{\\beta} \\left\\{ \\|y - X\\beta\\|^2_2 + \\lambda \\|\\beta\\|^2_2 \\right\\}$\n",
    "\n",
    "Where $\\lambda = \\frac{\\sigma^2}{\\tau^2}$ is the regularization parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/ridge.webp\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔑 **Key Concept:** The regularization parameter $\\lambda$ represents the ratio of the noise variance to the prior variance on weights. A larger $\\lambda$ implies more confidence in the prior (that weights should be close to zero) relative to the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_'></a>[How Ridge Regression Works](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression operates by shrinking the coefficients of correlated predictors towards each other, allowing them to borrow strength from one another. This has several important effects:\n",
    "\n",
    "1. **Coefficient Stabilization:** \n",
    "   In the presence of multicollinearity, ordinary least squares can produce wildly varying coefficients. Ridge regression stabilizes these coefficients, making them more reliable.\n",
    "\n",
    "2. **Variance Reduction:** \n",
    "   By constraining the coefficients, ridge regression reduces the model's variance, often at the cost of introducing a small amount of bias. This is particularly beneficial when the least squares estimates have high variance.\n",
    "\n",
    "3. **Improved Generalization:** \n",
    "   The coefficient shrinkage often leads to better performance on unseen data, as it prevents the model from fitting noise in the training data too closely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometrically, ridge regression can be viewed as constraining the coefficient vector to lie within a sphere centered at the origin. The radius of this sphere is determined by the regularization parameter $\\lambda$. This constraint ensures that no single feature dominates the model, promoting a more balanced use of features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_3_'></a>[Choosing the Regularization Parameter](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of $\\lambda$ is crucial in ridge regression:\n",
    "\n",
    "- $\\lambda = 0$: Equivalent to ordinary least squares (maximum likelihood estimation)\n",
    "- $\\lambda \\to \\infty$: All coefficients approach zero (except the intercept), reflecting complete trust in the prior\n",
    "- Optimal $\\lambda$: Typically chosen through cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 **Pro Tip:** Plot the coefficient paths (how coefficients change with $\\lambda$) to gain insights into feature importance and model behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ridge regression does not perform feature selection; all features are retained in the model. If feature selection is desired, Lasso regression (L1 regularization) can be a better choice (which we'll cover next).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some key advantages and limitations of Ridge Regression:\n",
    "\n",
    "**Advantages:**\n",
    "- Handles multicollinearity effectively\n",
    "- Often improves prediction accuracy on new data\n",
    "- Provides a continuous shrinkage of coefficients\n",
    "\n",
    "**Limitations:**\n",
    "- Does not perform feature selection (all features are kept in the model)\n",
    "- Scale-dependent: Features need to be standardized before applying ridge regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_4_'></a>[Implementing Ridge Regression](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, ridge regression can be easily implemented using popular machine learning libraries. Here's a simple example using scikit-learn:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the modified mini-batch gradient descent function with ridge regularization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def mini_batch_gradient_descent_with_ridge(X, y, learning_rate=0.01, num_epoch=1000, batch_size=20, lambda_reg=0.1):\n",
    "    n, m = X.shape\n",
    "    beta = np.random.randn(m, 1)\n",
    "    cost_history = []\n",
    "\n",
    "    for _ in range(num_epoch):\n",
    "        shuffled_indices = np.random.permutation(n)\n",
    "        X_shuffled = X[shuffled_indices]\n",
    "        y_shuffled = y[shuffled_indices]\n",
    "\n",
    "        for i in range(0, n, batch_size):\n",
    "            xi = X_shuffled[i:i+batch_size]\n",
    "            yi = y_shuffled[i:i+batch_size]\n",
    "            gradient = compute_gradient_with_ridge(xi, yi, beta, lambda_reg)\n",
    "            beta -= learning_rate * gradient\n",
    "\n",
    "        cost = compute_cost_with_ridge(X, y, beta, lambda_reg)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "    return beta, cost_history\n",
    "\n",
    "def compute_gradient_with_ridge(X, y, beta, lambda_reg):\n",
    "    m = X.shape[0]\n",
    "    predictions = X.dot(beta)\n",
    "    gradient = (1/m) * X.T.dot(predictions - y) + (lambda_reg/m) * beta\n",
    "    return gradient\n",
    "\n",
    "def compute_cost_with_ridge(X, y, beta, lambda_reg):\n",
    "    m = X.shape[0]\n",
    "    predictions = X.dot(beta)\n",
    "    cost = (1/(2*m)) * np.sum((predictions - y)**2) + (lambda_reg/(2*m)) * np.sum(beta**2)\n",
    "    return cost\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main changes in this version are:\n",
    "\n",
    "1. The function name is changed to `mini_batch_gradient_descent_with_ridge` to reflect the addition of ridge regularization.\n",
    "\n",
    "2. A new parameter `lambda_reg` is added to control the strength of the regularization.\n",
    "\n",
    "3. The `compute_gradient` and `compute_cost` functions are replaced with `compute_gradient_with_ridge` and `compute_cost_with_ridge`, respectively.\n",
    "\n",
    "4. In `compute_gradient_with_ridge`, the regularization term `(lambda_reg/m) * beta` is added to the gradient calculation.\n",
    "\n",
    "5. In `compute_cost_with_ridge`, the regularization term `(lambda_reg/(2*m)) * np.sum(beta**2)` is added to the cost calculation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ridge regularization adds a penalty term to the cost function, which helps prevent overfitting by discouraging large values in the beta coefficients. The `lambda_reg` parameter controls the strength of this regularization: a larger value will result in stronger regularization, while a value of 0 would be equivalent to no regularization. When using this function, you can adjust the `lambda_reg` parameter to find the right balance between fitting the training data and preventing overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Important Note:** Always standardize your features before applying ridge regression to ensure that the penalty is applied uniformly across all features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By understanding Ridge Regression from both its mathematical derivation and practical application, you can more effectively leverage this powerful technique to build robust and generalizable linear models, especially in scenarios with multicollinearity or high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_'></a>[Lasso Regression (L1 Regularization)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso (Least Absolute Shrinkage and Selection Operator) Regression is another powerful regularization technique used in linear regression. Unlike Ridge Regression, Lasso not only helps in preventing overfitting but also performs feature selection, making it particularly useful in high-dimensional datasets where only a subset of features are relevant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Ridge Regression, Lasso can be derived from a Bayesian perspective as a Maximum A Posteriori (MAP) estimation:\n",
    "\n",
    "1. **Likelihood**: We assume the same normal distribution for the target variable:\n",
    "\n",
    "   $y | X, \\beta \\sim \\mathcal{N}(X\\beta, \\sigma^2I)$\n",
    "\n",
    "2. **Prior**: Instead of a Gaussian prior, Lasso assumes a Laplace prior on the weights:\n",
    "\n",
    "   $p(\\beta) \\propto \\exp(-\\lambda\\|\\beta\\|_1)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the same MAP estimation process as with Ridge Regression, we arrive at the Lasso objective function:\n",
    "\n",
    "$\\min_{\\beta} \\left\\{ \\|y - X\\beta\\|^2_2 + \\lambda \\|\\beta\\|_1 \\right\\}$\n",
    "\n",
    "Where $\\|\\beta\\|_1 = \\sum_{j=1}^p |\\beta_j|$ is the L1 norm of the coefficient vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/lasso.webp\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔑 **Key Concept:** The use of the L1 norm in the penalty term leads to sparse solutions, effectively performing feature selection by pushing some coefficients exactly to zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/l1-l2.ppm\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_'></a>[How Lasso Regression Works](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso regression has some unique properties that distinguish it from Ridge regression:\n",
    "\n",
    "1. **Sparse Solutions**: \n",
    "   Lasso can produce sparse models by setting some coefficients exactly to zero. This is due to the geometry of the L1 penalty, which intersects with the error contours at corners, corresponding to zero values for some coefficients.\n",
    "\n",
    "2. **Feature Selection**: \n",
    "   By pushing coefficients to zero, Lasso effectively performs feature selection, identifying the most important predictors in the model.\n",
    "\n",
    "3. **Bias-Variance Trade-off**: \n",
    "   Like Ridge regression, Lasso reduces variance but may increase bias. However, the sparsity induced by Lasso can lead to simpler models that may generalize better in some cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometrically, the Lasso constraint can be visualized as a diamond-shaped region in 2D (or a cross-polytope in higher dimensions) centered at the origin. The optimization process finds the point where this constraint region first touches the contours of the least squares error function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_2_'></a>[The Path of Coefficients](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most insightful aspects of Lasso is the path of coefficients as $\\lambda$ varies:\n",
    "\n",
    "- As $\\lambda$ increases, more coefficients are pushed to exactly zero.\n",
    "- The order in which coefficients become non-zero (or return to zero) can provide insights into feature importance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 **Pro Tip:** Plotting the Lasso path (coefficients vs. $\\lambda$) can provide valuable insights into the relative importance of features and how the model changes with regularization strength.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_3_'></a>[Choosing the Regularization Parameter](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with Ridge regression, choosing the right $\\lambda$ is crucial:\n",
    "\n",
    "- $\\lambda = 0$: Equivalent to ordinary least squares\n",
    "- $\\lambda \\to \\infty$: All coefficients become zero\n",
    "- Optimal $\\lambda$: Typically chosen through cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_4_'></a>[Advantages and Limitations](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages:**\n",
    "- Performs feature selection, leading to simpler models\n",
    "- Can handle high-dimensional data effectively\n",
    "- Provides interpretable models by identifying key predictors\n",
    "\n",
    "**Limitations:**\n",
    "- Can be unstable when features are highly correlated\n",
    "- May not perform well when there are many relevant features with small effects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_5_'></a>[Implementing Lasso Regression](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the mini-batch gradient descent function modified to use Lasso regularization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def mini_batch_gradient_descent_with_lasso(X, y, learning_rate=0.01, num_epoch=1000, batch_size=20, lambda_reg=0.1):\n",
    "    n, m = X.shape\n",
    "    beta = np.random.randn(m, 1)\n",
    "    cost_history = []\n",
    "\n",
    "    for _ in range(num_epoch):\n",
    "        shuffled_indices = np.random.permutation(n)\n",
    "        X_shuffled = X[shuffled_indices]\n",
    "        y_shuffled = y[shuffled_indices]\n",
    "\n",
    "        for i in range(0, n, batch_size):\n",
    "            xi = X_shuffled[i:i+batch_size]\n",
    "            yi = y_shuffled[i:i+batch_size]\n",
    "            gradient = compute_gradient_with_lasso(xi, yi, beta, lambda_reg)\n",
    "            beta -= learning_rate * gradient\n",
    "\n",
    "        cost = compute_cost_with_lasso(X, y, beta, lambda_reg)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "    return beta, cost_history\n",
    "\n",
    "def compute_gradient_with_lasso(X, y, beta, lambda_reg):\n",
    "    m = X.shape[0]\n",
    "    predictions = X.dot(beta)\n",
    "    gradient = (1/m) * X.T.dot(predictions - y) + lambda_reg * np.sign(beta)\n",
    "    return gradient\n",
    "\n",
    "def compute_cost_with_lasso(X, y, beta, lambda_reg):\n",
    "    m = X.shape[0]\n",
    "    predictions = X.dot(beta)\n",
    "    cost = (1/(2*m)) * np.sum((predictions - y)**2) + lambda_reg * np.sum(np.abs(beta))\n",
    "    return cost\n",
    "\n",
    "def soft_thresholding(x, lambda_reg):\n",
    "    return np.sign(x) * np.maximum(np.abs(x) - lambda_reg, 0)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main changes for Lasso regularization are:\n",
    "\n",
    "1. The function name is changed to `mini_batch_gradient_descent_with_lasso`.\n",
    "\n",
    "2. In `compute_gradient_with_lasso`, the regularization term is now `lambda_reg * np.sign(beta)`. This is because the derivative of the L1 norm (used in Lasso) is the sign function.\n",
    "\n",
    "3. In `compute_cost_with_lasso`, the regularization term is now `lambda_reg * np.sum(np.abs(beta))`. This is the L1 norm of the coefficients.\n",
    "\n",
    "4. A new function `soft_thresholding` is added. This function can be used to implement coordinate descent for Lasso, which is often more efficient than gradient descent for Lasso problems. However, it's not used in the main function here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Lasso regularization tends to produce sparse solutions (i.e., it can drive some coefficients to exactly zero), which can be useful for feature selection. The `lambda_reg` parameter controls the strength of the regularization, with larger values producing sparser solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important consideration with Lasso is that the objective function is not differentiable at zero, which can sometimes cause issues with gradient-based optimization. In practice, this is often handled by using specialized optimization algorithms (like coordinate descent) or by using a smooth approximation of the L1 norm. The implementation provided here uses the standard gradient descent approach, which may not always converge to the optimal solution for Lasso problems, especially with high-dimensional data or large `lambda_reg` values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Important Note:** Like Ridge regression, it's important to standardize features before applying Lasso to ensure fair penalization across all features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_6_'></a>[Practical Considerations](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Feature Correlation**: When features are highly correlated, Lasso tends to pick one arbitrarily. Consider using Elastic Net (a combination of L1 and L2 penalties) in such cases.\n",
    "\n",
    "2. **Stability**: The feature selection property of Lasso can be unstable across different samples. Techniques like stability selection can be used to improve reliability.\n",
    "\n",
    "3. **Interpretability**: While Lasso provides a form of feature importance, be cautious about interpreting the magnitude of non-zero coefficients directly as feature importance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By understanding the principles behind Lasso regression and its unique properties, you can effectively leverage this technique for both regularization and feature selection in your linear models, particularly in high-dimensional settings where identifying key predictors is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_'></a>[Elastic Net Regularization](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net regularization is a hybrid approach that combines the strengths of both Ridge (L2) and Lasso (L1) regularization. It was developed to address some of the limitations of Lasso, particularly in scenarios with highly correlated features or when the number of predictors significantly exceeds the number of observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net aims to strike a balance between the L1 and L2 penalties, offering a more flexible and robust regularization technique. It introduces two hyperparameters:\n",
    "\n",
    "1. $\\alpha$: Controls the overall strength of regularization\n",
    "2. $\\rho$: Determines the mix of L1 and L2 penalties\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/elastic-net.webp\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_1_'></a>[Mathematical Formulation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Elastic Net objective function is defined as:\n",
    "\n",
    "$\\min_{\\beta} \\left\\{ \\|y - X\\beta\\|^2_2 + \\alpha \\left( \\rho \\|\\beta\\|_1 + \\frac{1-\\rho}{2} \\|\\beta\\|^2_2 \\right) \\right\\}$\n",
    "\n",
    "Where:\n",
    "- $\\|\\beta\\|_1$ is the L1 norm (sum of absolute values)\n",
    "- $\\|\\beta\\|^2_2$ is the squared L2 norm\n",
    "- $\\alpha \\geq 0$ is the overall regularization strength\n",
    "- $0 \\leq \\rho \\leq 1$ is the mixing parameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔑 **Key Concept:** When $\\rho = 1$, Elastic Net becomes Lasso; when $\\rho = 0$, it becomes Ridge regression. Values in between create a compromise between the two.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a Bayesian perspective, Elastic Net can be seen as placing a prior on the coefficients that is a mixture of Laplace (for L1) and Gaussian (for L2) distributions. This combined prior allows for both sparsity and the grouping effect of correlated features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_2_'></a>[How Elastic Net Works](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net combines the beneficial properties of both Ridge and Lasso:\n",
    "\n",
    "1. **Feature Selection**: \n",
    "   Like Lasso, Elastic Net can produce sparse models by setting some coefficients to exactly zero, especially when $\\rho$ is close to 1.\n",
    "\n",
    "2. **Handling Correlated Features**: \n",
    "   Unlike Lasso, which tends to arbitrarily select one feature from a group of correlated features, Elastic Net can select groups of correlated features together.\n",
    "\n",
    "3. **Stability**: \n",
    "   The L2 penalty provides stability, especially in scenarios where predictors are highly correlated, addressing a key limitation of Lasso.\n",
    "\n",
    "4. **Overcome Limitations**: \n",
    "   Elastic Net can handle situations where the number of predictors (p) is much larger than the number of observations (n), a scenario where Lasso is limited to selecting at most n variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_3_'></a>[Choosing Hyperparameters](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting appropriate values for $\\alpha$ and $\\rho$ is crucial for Elastic Net's performance:\n",
    "\n",
    "- $\\alpha$: Controls overall regularization strength. Larger values increase regularization.\n",
    "- $\\rho$: Balances L1 and L2 penalties. Values closer to 1 favor sparsity, while values closer to 0 favor Ridge-like behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 **Pro Tip:** Use cross-validation with a grid search over different combinations of $\\alpha$ and $\\rho$ to find the optimal hyperparameters for your specific dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_4_'></a>[Advantages and Limitations](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages:**\n",
    "- Combines benefits of both L1 and L2 regularization\n",
    "- Handles correlated features better than Lasso\n",
    "- Can perform feature selection while still maintaining some contribution from all features\n",
    "\n",
    "**Limitations:**\n",
    "- Requires tuning of two hyperparameters instead of one\n",
    "- May be computationally more intensive due to the additional hyperparameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_5_'></a>[Implementing Elastic Net](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of implementing Elastic Net using scikit-learn:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Here's the mini-batch gradient descent function modified to use Elastic Net regularization, which combines both L1 (Lasso) and L2 (Ridge) regularization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def mini_batch_gradient_descent_with_elastic_net(X, y, learning_rate=0.01, num_epoch=1000, batch_size=20, lambda1=0.1, lambda2=0.1):\n",
    "    n, m = X.shape\n",
    "    beta = np.random.randn(m, 1)\n",
    "    cost_history = []\n",
    "\n",
    "    for _ in range(num_epoch):\n",
    "        shuffled_indices = np.random.permutation(n)\n",
    "        X_shuffled = X[shuffled_indices]\n",
    "        y_shuffled = y[shuffled_indices]\n",
    "\n",
    "        for i in range(0, n, batch_size):\n",
    "            xi = X_shuffled[i:i+batch_size]\n",
    "            yi = y_shuffled[i:i+batch_size]\n",
    "            gradient = compute_gradient_with_elastic_net(xi, yi, beta, lambda1, lambda2)\n",
    "            beta -= learning_rate * gradient\n",
    "\n",
    "        cost = compute_cost_with_elastic_net(X, y, beta, lambda1, lambda2)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "    return beta, cost_history\n",
    "\n",
    "def compute_gradient_with_elastic_net(X, y, beta, lambda1, lambda2):\n",
    "    m = X.shape[0]\n",
    "    predictions = X.dot(beta)\n",
    "    l1_term = lambda1 * np.sign(beta)\n",
    "    l2_term = lambda2 * beta\n",
    "    gradient = (1/m) * X.T.dot(predictions - y) + l1_term + l2_term\n",
    "    return gradient\n",
    "\n",
    "def compute_cost_with_elastic_net(X, y, beta, lambda1, lambda2):\n",
    "    m = X.shape[0]\n",
    "    predictions = X.dot(beta)\n",
    "    l1_penalty = lambda1 * np.sum(np.abs(beta))\n",
    "    l2_penalty = (lambda2 / 2) * np.sum(beta**2)\n",
    "    cost = (1/(2*m)) * np.sum((predictions - y)**2) + l1_penalty + l2_penalty\n",
    "    return cost\n",
    "\n",
    "def soft_thresholding(x, lambda1):\n",
    "    return np.sign(x) * np.maximum(np.abs(x) - lambda1, 0)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main changes for Elastic Net regularization are:\n",
    "\n",
    "1. The function name is changed to `mini_batch_gradient_descent_with_elastic_net`.\n",
    "\n",
    "2. We now have two regularization parameters: `lambda1` for the L1 (Lasso) term and `lambda2` for the L2 (Ridge) term.\n",
    "\n",
    "3. In `compute_gradient_with_elastic_net`, we include both the L1 term (`lambda1 * np.sign(beta)`) and the L2 term (`lambda2 * beta`) in the gradient calculation.\n",
    "\n",
    "4. In `compute_cost_with_elastic_net`, we include both the L1 penalty (`lambda1 * np.sum(np.abs(beta))`) and the L2 penalty (`(lambda2 / 2) * np.sum(beta**2)`) in the cost calculation.\n",
    "\n",
    "5. The `soft_thresholding` function is kept, as it can be useful for coordinate descent implementations of Elastic Net, although it's not used in the main function here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net regularization combines the benefits of both Lasso and Ridge regularization:\n",
    "\n",
    "- Like Lasso, it can produce sparse models by driving some coefficients to exactly zero, which is useful for feature selection.\n",
    "- Like Ridge, it can handle correlated features well and doesn't have limitations in high-dimensional settings where p > n (number of features greater than number of samples).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The balance between L1 and L2 regularization can be adjusted by changing the relative values of `lambda1` and `lambda2`:\n",
    "\n",
    "- If `lambda1 = 0`, it reduces to Ridge regression.\n",
    "- If `lambda2 = 0`, it reduces to Lasso regression.\n",
    "- When both are non-zero, you get the benefits of both regularization techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with Lasso, the non-differentiability of the L1 term at zero can sometimes cause issues with gradient-based optimization. More sophisticated optimization techniques (like coordinate descent or proximal gradient methods) are often used for Elastic Net in practice, especially for high-dimensional problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Important Note:** As with Ridge and Lasso, it's crucial to standardize features before applying Elastic Net to ensure fair penalization across all features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_6_'></a>[Practical Considerations](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Interpretation**: While Elastic Net provides a form of feature importance, be cautious about directly interpreting the magnitude of non-zero coefficients as feature importance.\n",
    "\n",
    "2. **Computational Cost**: The additional hyperparameter can make the tuning process more computationally intensive compared to Ridge or Lasso alone.\n",
    "\n",
    "3. **Model Complexity**: Elastic Net models can be more complex than pure Lasso models but may offer better predictive performance, especially with correlated features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By leveraging Elastic Net regularization, you can create models that benefit from both the sparsity of Lasso and the stability of Ridge regression. This makes Elastic Net a versatile choice, particularly useful in scenarios with complex feature interactions or when dealing with high-dimensional data where feature selection is desirable but pure Lasso might be too aggressive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_'></a>[Impact of Regularization on Bias and Variance](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding how regularization affects the bias-variance trade-off is crucial for effectively applying these techniques in practice. This section explores how different regularization methods impact model bias and variance, providing insights into when and why to use each approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into the impact of regularization, let's briefly recap the bias-variance trade-off:\n",
    "\n",
    "- **Bias**: The error introduced by approximating a real-world problem with a simplified model.\n",
    "- **Variance**: The model's sensitivity to fluctuations in the training data.\n",
    "- **Trade-off**: As model complexity increases, bias tends to decrease while variance increases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of regularization is to find an optimal balance between bias and variance to minimize overall prediction error. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization techniques generally work by introducing a controlled amount of bias to reduce variance:\n",
    "\n",
    "1. **Reducing Variance**: \n",
    "   By constraining model parameters, regularization reduces the model's sensitivity to individual data points, lowering variance.\n",
    "\n",
    "2. **Increasing Bias**: \n",
    "   The constraints imposed by regularization can prevent the model from perfectly fitting the training data, potentially increasing bias.\n",
    "\n",
    "3. **Overall Error**: \n",
    "   The aim is to decrease variance more than the increase in bias, reducing overall prediction error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/ridge-lasso-elastic.webp\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc6_1_'></a>[Impact of Ridge Regression (L2)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression impacts bias and variance in the following ways:\n",
    "\n",
    "1. **Continuous Shrinkage**: \n",
    "   - Ridge shrinks all coefficients towards zero, but rarely sets them exactly to zero.\n",
    "   - This results in a more stable model with lower variance.\n",
    "\n",
    "2. **Bias Introduction**: \n",
    "   - As λ (regularization strength) increases, bias generally increases.\n",
    "   - The model becomes simpler and may underfit if λ is too large.\n",
    "\n",
    "3. **Variance Reduction**: \n",
    "   - Larger λ values lead to more significant variance reduction.\n",
    "   - This is particularly beneficial when features are correlated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔍 **Example:** In a dataset with many correlated features, Ridge can significantly reduce variance by distributing the impact across all features, rather than relying heavily on a subset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc6_2_'></a>[Impact of Lasso Regression (L1)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso affects bias and variance differently from Ridge:\n",
    "\n",
    "1. **Feature Selection**: \n",
    "   - Lasso can set some coefficients exactly to zero, effectively performing feature selection.\n",
    "   - This can lead to simpler models with potentially lower variance.\n",
    "\n",
    "2. **Sparse Solutions**: \n",
    "   - As λ increases, more coefficients are set to zero, increasing model sparsity.\n",
    "   - This can result in more interpretable models but may increase bias if important features are excluded.\n",
    "\n",
    "3. **Variance Reduction**: \n",
    "   - By selecting a subset of features, Lasso can significantly reduce variance, especially in high-dimensional spaces.\n",
    "\n",
    "4. **Bias-Variance Balance**: \n",
    "   - The feature selection property of Lasso can lead to a different bias-variance trade-off compared to Ridge.\n",
    "   - It may achieve lower variance but potentially higher bias, especially if the true model is not sparse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 **Pro Tip:** Lasso can be particularly effective when you believe only a subset of features are truly relevant to the prediction task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc6_3_'></a>[Impact of Elastic Net](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net combines the properties of both Ridge and Lasso:\n",
    "\n",
    "1. **Flexible Trade-off**: \n",
    "   - By adjusting the mixing parameter (ρ), Elastic Net can achieve a balance between the bias-variance characteristics of Ridge and Lasso.\n",
    "\n",
    "2. **Grouped Selection**: \n",
    "   - In scenarios with groups of correlated features, Elastic Net can select or reject features as groups.\n",
    "   - This can lead to models with lower variance than Lasso while maintaining some of the interpretability benefits.\n",
    "\n",
    "3. **Stability**: \n",
    "   - Elastic Net tends to be more stable than Lasso in the presence of highly correlated features.\n",
    "   - This stability often translates to a more favorable bias-variance trade-off in such scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc6_4_'></a>[Practical Implications](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding these impacts helps in choosing and tuning regularization methods:\n",
    "\n",
    "1. **Feature Correlation**: \n",
    "   - With highly correlated features, Ridge or Elastic Net may be preferable to Lasso.\n",
    "   - They handle multicollinearity better, leading to more stable models with lower variance.\n",
    "\n",
    "2. **High-Dimensional Data**: \n",
    "   - In scenarios with many features relative to observations, Lasso or Elastic Net can be beneficial.\n",
    "   - Their feature selection properties can lead to simpler models with lower variance.\n",
    "\n",
    "3. **Model Interpretability**: \n",
    "   - If model interpretability is crucial, Lasso or Elastic Net with a higher L1 ratio might be preferred.\n",
    "   - The sparsity they induce can make it easier to identify key predictors.\n",
    "\n",
    "4. **Tuning Process**: \n",
    "   - When tuning regularization parameters, monitor both training and validation performance.\n",
    "   - The optimal regularization strength is often where validation performance peaks, balancing bias and variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Important Note:** The impact of regularization on bias and variance can vary depending on the specific dataset and problem. Always validate your approach using cross-validation and consider the practical implications of your model choices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc6_5_'></a>[Visualization of the Impact](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To truly understand the impact of regularization on bias and variance, it's often helpful to visualize:\n",
    "\n",
    "1. **Learning Curves**: Plot training and validation errors against the regularization strength.\n",
    "2. **Coefficient Paths**: Show how coefficients change as regularization strength increases.\n",
    "3. **Prediction Error Decomposition**: Visualize how total error, bias, and variance change with regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By carefully considering how different regularization techniques affect the bias-variance trade-off, you can make more informed decisions about which method to use and how to tune it for your specific problem. This understanding is key to building models that generalize well to unseen data, striking the right balance between model complexity and predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc7_'></a>[Summary and Key Takeaways](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we conclude our exploration of regularization techniques in linear regression, let's recap the main concepts and highlight the key takeaways. This summary will help solidify your understanding and provide a quick reference for applying these techniques in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is a crucial concept in machine learning that addresses overfitting by adding a penalty term to the loss function. We've explored three main regularization techniques:\n",
    "\n",
    "1. Ridge Regression (L2)\n",
    "2. Lasso Regression (L1)\n",
    "3. Elastic Net (Combination of L1 and L2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary goal of regularization is to create models that generalize well to unseen data by finding an optimal balance between bias and variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the general steps for implementing regularization:\n",
    "\n",
    "1. Preprocess your data (scaling, handling missing values, etc.)\n",
    "2. Split your data into training and testing sets.\n",
    "3. Choose a regularization technique based on your problem characteristics.\n",
    "4. Use cross-validation to tune hyperparameters.\n",
    "5. Train your model on the entire training set using the best hyperparameters.\n",
    "6. Evaluate on the test set to assess generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always compare your regularized models against a baseline (e.g., ordinary least squares) to ensure you're gaining benefits from regularization. Regularization is not a one-size-fits-all solution, so experiment with different techniques and hyperparameters to find the best fit for your data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is a powerful tool in the data scientist's toolkit, offering ways to create more robust and generalizable models. By understanding the principles behind these techniques and their impact on model behavior, you can make informed decisions about when and how to apply regularization in your projects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Important Note:** While regularization is highly effective, it's not a silver bullet. Always consider the specific characteristics of your data and the requirements of your problem when choosing and applying regularization techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you move forward in your machine learning journey, remember that the concepts learned here extend beyond linear regression. Many advanced techniques, including neural networks and other complex models, use similar principles of regularization to improve performance and generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By mastering these regularization techniques, you've gained valuable skills that will serve you well in a wide range of machine learning applications. Keep practicing, experimenting with different datasets, and staying curious about new developments in the field!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
